{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139236bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 1000]            768                  True\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              True\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "├─Sequential (heads)                                         [1, 768]             [1, 1000]            --                   True\n",
       "│    └─Linear (head)                                         [1, 768]             [1, 1000]            769,000              True\n",
       "============================================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 173.23\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 336.96\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vit_b_16\n",
    "from torchinfo import summary\n",
    "\n",
    "vit_model = vit_b_16(weights = \"DEFAULT\")\n",
    "summary(vit_model, \n",
    "        input_size=(1, 3, 224, 224), # (batch, C = 3, H, W)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4362453",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model # get input and output functions from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, inference_mode\n",
    "\n",
    "# These steps must be followed in this exact order\n",
    "# -------------------------------------------------\n",
    "\n",
    "# 1. get the pretraind weights of the original conv2d block (input)\n",
    "old_conv = vit_model.conv_proj\n",
    "\n",
    "# 2. create the new conv2d input block, change the input to take 1 channel only (grayscale)\n",
    "new_conv = nn.Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
    "\n",
    "# 3. average the rgb weights across channels to form one grayscale channel\n",
    "with inference_mode():\n",
    "    new_conv.weight[:] = old_conv.weight.mean(dim=1, keepdim=True)\n",
    "    new_conv.bias[:] = old_conv.bias\n",
    "    \n",
    "# 4. replace the input block\n",
    "vit_model.conv_proj = new_conv\n",
    "\n",
    "# 5. freeze model\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# change the output to 2 classes only (healthy, pd)\n",
    "# this only unfreezes this block\n",
    "vit_model.heads = nn.Sequential(\n",
    "    nn.Linear(in_features=768, out_features=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18a642f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 1, 224, 224]     [1, 2]               768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 1, 224, 224]     [1, 768, 14, 14]     (197,376)            False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "├─Sequential (heads)                                         [1, 768]             [1, 2]               --                   True\n",
       "│    └─Linear (0)                                            [1, 768]             [1, 2]               1,538                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,406,978\n",
       "Trainable params: 1,538\n",
       "Non-trainable params: 85,405,440\n",
       "Total mult-adds (M): 95.39\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 227.63\n",
       "Estimated Total Size (MB): 331.91\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vit_model, \n",
    "        input_size=(1, 1, 224, 224), # (batch, C = 1 , H, W)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb8c496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import ViT_B_16_Weights\n",
    "ViT_B_16_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795a69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 1, 224, 224]     [1, 2]               768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 1, 224, 224]     [1, 768, 14, 14]     (197,376)            False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "├─Sequential (heads)                                         [1, 768]             [1, 2]               --                   True\n",
       "│    └─Linear (0)                                            [1, 768]             [1, 2]               1,538                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,406,978\n",
       "Trainable params: 1,538\n",
       "Non-trainable params: 85,405,440\n",
       "Total mult-adds (M): 95.39\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 227.63\n",
       "Estimated Total Size (MB): 331.91\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_Vit_b16 import create_vit\n",
    "from torchinfo import summary\n",
    "\n",
    "model = create_vit()\n",
    "\n",
    "summary(model, \n",
    "        input_size=(1, 1, 224, 224), # (batch, C = 1 , H, W)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d3f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
